/**
 * 写入文件
 * hadoop jar XXX.jar com.hdfsclient.FileCopyFromLocal
 */

package com.hdfsclient;

import java.io.BufferedInputStream;
import java.io.FileInputStream;
import java.io.IOException;
import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

public class FileCopyFromLocal {

	public static void main(String[] args) throws IOException {
		//本地文件路径
		String source = "/home/hadoop/test";
		//远程目标文件路径
		String destination = "hdfs://master:9000/user/hadoop/test2";
		
		BufferedInputStream in = new BufferedInputStream(new FileInputStream(source));
		Configuration conf = new Configuration();
		
		//FileSystem 实例的create()方法返回FSDataOutputStream对象，与FSDataInputStream类不同的是，FSDataOutputStream不允许在文件中定位，
		//这是因为HDFS只允许一个已打开的文件顺序写入，或者在现有文件的末尾追加数据
		
		FileSystem fs = FileSystem.get(URI.create(destination), conf);
		FSDataOutputStream out = fs.create(new Path(destination));
		IOUtils.copyBytes(in, out, 4096, true);
	}
}
